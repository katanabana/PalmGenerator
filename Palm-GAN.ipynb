{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772a8d5-c65e-41e2-b4dc-0c2e5b05371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_dataset(image_dir='data/images', csv_file='data/info.csv', output_dir='preprocessed', target_size=(128, 128)):\n",
    "    # Load dataset info\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Filter relevant images: 'palmar left' or 'palmar right'\n",
    "    filtered_df = df[df['aspectOfHand'].isin(['palmar left', 'palmar right'])]\n",
    "\n",
    "    # Create output directory if not exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Resize and save images\n",
    "    for _, row in tqdm(filtered_df.iterrows(), desc='transforming', total=filtered_df.shape[0]):\n",
    "        img_name = row['imageName']\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # Ensure 24-bit color\n",
    "        \n",
    "        # Resize image to target size\n",
    "        img_resized = img.resize(target_size)\n",
    "        \n",
    "        # Save preprocessed image\n",
    "        img_resized.save(os.path.join(output_dir, img_name))\n",
    "\n",
    "    print(f\"Preprocessed dataset saved in {output_dir}\")\n",
    "\n",
    "# Run preprocessing\n",
    "preprocess_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a03e34-f16c-40d1-a59c-681dd992e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the images and create a ZIP archive for training\n",
    "!python stylegan2-ada-pytorch/dataset_tool.py --source=preprocessed --dest=palm_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfddecc-cc84-4e9c-871e-7b379a038e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python stylegan2-ada-pytorch/train.py --outdir=training-runs --data=palm_dataset.zip --gpus=8 --cfg=stylegan2 --aug=ada --metrics=fid50k_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157db3e4-e974-4dbf-bfc8-f8dbc6127e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from typing import Optional\n",
    "\n",
    "def generate_palm_images(output_dir: str, num_images: int, model_path: str, \n",
    "                         age: Optional[int] = None, gender: Optional[str] = None,\n",
    "                         skin_color: Optional[str] = None, accessories: Optional[int] = None, \n",
    "                         side: Optional[str] = None):\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Load the trained StyleGAN2-ADA model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    with torch.no_grad():\n",
    "        G = torch.load(model_path).to(device)  # Load generator model\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        z = torch.randn(1, G.z_dim).to(device)  # Latent vector\n",
    "\n",
    "        # Apply conditioning based on optional parameters (age, gender, skin color, etc.)\n",
    "        conditioning = {\n",
    "            'age': age,\n",
    "            'gender': gender,\n",
    "            'skin_color': skin_color,\n",
    "            'accessories': accessories,\n",
    "            'side': side\n",
    "        }\n",
    "        # Assume that the generator is conditioned on these attributes, map them to latent codes or style inputs\n",
    "        # You need to design the latent manipulation based on your specific conditioning logic.\n",
    "\n",
    "        img = G(z, conditioning)  # Generate an image\n",
    "\n",
    "        # Convert image to PIL format and save\n",
    "        img = (img.clamp(-1, 1).cpu().numpy() * 127.5 + 127.5).astype(np.uint8)\n",
    "        img = Image.fromarray(img[0].transpose(1, 2, 0))  # Convert from CHW to HWC\n",
    "        img.save(os.path.join(output_dir, f\"palm_{i+1}.png\"))\n",
    "\n",
    "    print(f\"Generated {num_images} images in {output_dir}\")\n",
    "\n",
    "# Example usage\n",
    "generate_palm_images(output_dir='generated_palm_images', num_images=10, model_path='path/to/your_trained_model.pkl', \n",
    "                     age=30, gender='male', skin_color='medium', accessories=1, side='left')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
